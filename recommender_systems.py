# -*- coding: utf-8 -*-
"""Recommender Systems.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18bvLev93BIh0YMkTcXhhcLcZ49iyXYX3
"""

import pandas as pd
import numpy as np
from collections import defaultdict
from math import sqrt
import pickle
import time

MAX_USERS = 6040
MAX_ITEMS = 3952

def read_data():
    ratings = defaultdict(dict)
    with open('ratings.dat', 'r') as file:
        for line in file:
            user_id,movie_id,rating_value,timestamp = [*map(int, line.split('::'))]
            ratings[user_id][movie_id] = rating_value
    return ratings

def normalise(ratings):
    averages = defaultdict()
    for user,movie_ratings in ratings.items():
        mean = sum(movie_ratings.values()) / len(movie_ratings.values())
        for movie_id in movie_ratings:
            movie_ratings[movie_id] -= mean
        averages[user] = mean
    return ratings,averages

def RMSE(pred, truth):
    '''
    Calculate Root Mean Square Error (RMSE).
    Inputs:
    pred (1D numpy array): numpy array containing predicted values.
    truth (1D numpy array): numpy array containing the ground truth values.
    Returns:
    rmse (float): The Root Mean Square Error.
    '''
    return np.sqrt(np.sum(np.square(pred-truth)/float(pred.shape[0])))

def RMSE_mat(matA, matB):
    '''
    Calculate Root Mean Square Error (RMSE) between two matrices. Mainly used
    to find error original and reconstructed matrices while working with 
    matrix decompositions.
    Inputs:
    matA (2D numpy array): Matrix A 
    matB (2D numpy array): Matrix B

    Returns:
    rmse (float): Root Mean Square Error.
    '''
    return np.sqrt(np.sum(np.square(matA-matB))/(matA.shape[0]*matA.shape[1]))

def MAE(matA, matB):
    return np.sum(np.abs(matA-matB)) /(matA.shape[0]*matA.shape[1])

_ratings = read_data()
n_ratings,avs = normalise(_ratings)

"""# COLLABORATIVE FILTERING"""

class Collaborative_filtering:
    
    def __init__(self,Ratings,K,num_users = 50,num_movies = 100):
        self.R = Ratings
        self.ratings,self.means = normalise(Ratings)
        self.K = K
        self.num_users = num_users
        self.num_movies = num_movies
    
    def predict(self,user_id, movie_id, N): #Collaborative filtering
        score = []
        det1 = sqrt(sum([i*i for i in self.ratings[user_id].values()]))
        for i in range(1,MAX_USERS + 1):
            if user_id == i or movie_id not in self.ratings[i]:
                continue

            num = 0
            for k,v in self.ratings[user_id].items():
                if k in self.ratings[i]:
                    num += v * self.ratings[i][k]
            den = det1 * sqrt(sum([j*j for j in self.ratings[i].values()]))
            
            score.append((num/den, self.ratings[i][movie_id]))
        score.sort()
        if len(score) > N:
            score = score[:N]
        return sum([i[0] * i[1] for i in score]) / sum([i[0] for i in score])
    
    def evaluate(self):
        start_time = time.time()
        pred = []
        truth = []
        for uid in range(1,self.num_users+1):
            print('User number ', uid, end='        \r')
            for mid in range(1,self.num_movies+1):
                if mid in self.ratings[uid]:
                    truth.append( self.ratings[uid][mid] )#changes
                    pred.append(self.predict(uid, mid, self.K) )#changes
        truth = np.asarray(truth).reshape(1,len(truth))
        pred = np.asarray(pred).reshape(1,len(pred))
        print('rmse : ' ,RMSE_mat(pred,truth))
        print('mae : ',MAE(pred,truth))
        end_time = time.time()
        average_time  = (end_time - start_time) / pred.shape[1]
        print('number of predictions made',pred.shape[1])
        print('average time per prediction',average_time)
        print('total time',end_time - start_time)

col = Collaborative_filtering(_ratings,100)
col.evaluate()

"""# Collaborative with Baseline"""

class Collaborative_Baseline:
    def __init__(self,Ratings,K,num_users = 50,num_movies = 100):
        self.R = Ratings
        self.ratings,self.means = normalise(Ratings)
        self.K = K
        self.num_users = num_users
        self.num_movies = num_movies
        self.u = 0
        self.b_user = defaultdict(float)
        self.b_movie = defaultdict(float)
    
    def precompute_with_baseline(self):
        cnt = 0
        movie_cnt = defaultdict(float)
        for user,movie_ratings in self.ratings.items():
            self.b_user[user] = 0
            for movie_id,rating in movie_ratings.items():            
                self.b_user[user] += rating
                self.u += rating
                cnt += 1
                if movie_id not in movie_cnt:
                    movie_cnt[movie_id] = 0
                movie_cnt[movie_id] += 1
                if movie_id not in self.b_movie:
                    self.b_movie[movie_id] = 0
                self.b_movie[movie_id] += rating
            self.b_user[user] = self.b_user[user] / len(movie_ratings)
        self.u /= cnt
        for k,v in movie_cnt.items():
            self.b_movie[k] = self.b_movie[k] / v
            self.b_movie[k] = self.b_movie[k] - self.u
        for k in self.b_user:
            self.b_user[k] = self.b_user[k] - self.u
    
    def predict_with_baseline(self,user_id, movie_id, N):
        score = []
        det1 = sqrt(sum([i*i for i in self.ratings[user_id].values()]))
        for i in range(1,MAX_USERS+1):
            if user_id == i or movie_id not in self.ratings[i]:
                continue
            num = 0
            for k,v in self.ratings[user_id].items():
                if k in self.ratings[i]:
                    num += v * self.ratings[i][k]
            den = det1 * sqrt(sum([j*j for j in self.ratings[i].values()]))
            score.append((num/den, self.ratings[i][movie_id]))
        score.sort()
        if len(score) > N:
            score = score[:N]
        return self.u + self.b_user[user_id] + self.b_movie[movie_id] + sum([i[0] * i[1] for i in score]) / sum([i[0] for i in score])
    
    def evaluate(self):
        start_time = time.time()
        self.precompute_with_baseline()
        pred = []
        truth = []
        for uid in range(1,self.num_users+1):
            print('User number ', uid, end='        \r')
            for mid in range(1,self.num_movies+1):
                if mid in self.ratings[uid]:
                    truth.append( self.ratings[uid][mid] )#changes
                    pred.append(self.predict_with_baseline(uid, mid, self.K) )#changes
                    
        truth = np.asarray(truth).reshape(1,len(truth))
        pred = np.asarray(pred).reshape(1,len(pred))
        print('rmse : ' ,RMSE_mat(pred,truth))
        print('mae : ',MAE(pred,truth))
        end_time = time.time()
        average_time  = (end_time - start_time) / pred.shape[1]
        print('number of predictions made',pred.shape[1])
        print('average time per prediction',average_time)
        print('total time',end_time - start_time)

col = Collaborative_Baseline(_ratings,100)
col.evaluate()

"""# Single Value Decomposition - Top K"""

class SVD_topk:
    """ Class to implement Singular Value Decomposition
    """
    def __init__(self, matrix, K):
        self.K = K
        self.matrix = matrix
        self.MAX_USERS = len(self.matrix)
        self.MAX_ITEMS = len(self.matrix[0])
        self.findU()
        self.findV()
        self.findSigma()
        self.tryDimReduction()
        self.multiply()

    def findU(self):
        """ finds the U matrix for SVD decomposition
        """
        A = np.array(self.matrix)
        AAt = np.dot(A,A.T)
        self.eigen_values_AAt, self.eigen_vectors_AAt = np.linalg.eigh(AAt)
        self.rank_AAt = np.linalg.matrix_rank(AAt)
        self.index_AAt = sorted(range(len(self.eigen_values_AAt)),
                             key=lambda k: self.eigen_values_AAt[k], reverse=True)[:self.rank_AAt]
        self.U = np.zeros(shape=(self.MAX_USERS,self.rank_AAt))
        self.eigen_values_AAt = self.eigen_values_AAt[::-1] 
        self.eigen_values_AAt = self.eigen_values_AAt[:self.rank_AAt]

        for i in range(self.rank_AAt):
            self.U[:,i] = self.eigen_vectors_AAt[:,self.index_AAt[i]]

    def findSigma(self):
        """ finds the sigma matrix for SVD decomposition
        """
        self.sigma = np.zeros(shape=(self.rank_AAt, self.rank_AAt))
        for i in range(self.rank_AAt):
            self.sigma[i,i] = self.eigen_values_AAt[i] ** 0.5

    def findV(self):
        """ finds the V matrix for SVD decomposition
        """
        A = np.array(self.matrix)
        AtA = (A.T).dot(A)
        self.eigen_values_AtA, self.eigen_vectors_AtA = np.linalg.eigh(AtA)
        self.rank_AtA = np.linalg.matrix_rank(AtA)
        self.index_AtA = sorted(range(len(self.eigen_values_AtA)), key=lambda k: self.eigen_values_AtA[k], reverse=True)[:self.rank_AtA]
        self.V = np.zeros(shape=(self.MAX_ITEMS,self.rank_AtA))
        self.eigen_values_AtA = self.eigen_values_AtA[::-1]
        self.eigen_values_AtA = self.eigen_values_AtA[:self.rank_AtA]

        for i in range(self.rank_AtA):
            self.V[:,i] = self.eigen_vectors_AtA[:,self.index_AtA[i]]
        self.V = self.V.T

    def tryDimReduction(self):
        dk = len(self.sigma) - self.K
        self.sigma = self.sigma[:self.K, :self.K]
        self.U = self.U[:,:-dk]
        self.V = self.V[:-dk,:]

    def multiply(self):
        """ Multiplies the U, V and sigma matrices to get the matrix of predicted ratings.
        """
        self.result = np.dot((np.dot(self.U, self.sigma)), self.V)

N_users = 1000
N_movies = 1000
matrix = np.zeros(shape=(N_users, N_movies))
for i in range(N_users):
    for j in range(N_movies):
        if i in _ratings and j in n_ratings[i]:
            matrix[i-1][j-1] = n_ratings[i][j]
            
start_time = time.time()
svd = SVD_topk(matrix,7)
pred = svd.result
rmse = RMSE_mat(pred,matrix)
mae = MAE(pred,matrix)
end_time = time.time()
average_time  = (end_time - start_time) / (N_users * N_movies)
print('rmse : ' ,rmse)
print('mae : ',mae)
print('number of predictions made',(N_users * N_movies))
print('average time per prediction',average_time)
print('total time',(end_time - start_time))

print(svd.sigma)



"""# Single Value Decomposition - 90%"""

class SVD:
    """ Class to implement Singular Value Decomposition
    """
    def __init__(self, matrix):
        self.matrix = matrix
        self.MAX_USERS = len(self.matrix)
        self.MAX_ITEMS = len(self.matrix[0])
        self.findU()
        self.findV()
        self.findSigma()
        self.tryDimReduction()
        self.multiply()

    def findU(self):
        """ finds the U matrix for SVD decomposition
        """
        A = np.array(self.matrix)
        AAt = np.dot(A,A.T)
        self.eigen_values_AAt, self.eigen_vectors_AAt = np.linalg.eigh(AAt)
        self.rank_AAt = np.linalg.matrix_rank(AAt)
        self.index_AAt = sorted(range(len(self.eigen_values_AAt)),
                             key=lambda k: self.eigen_values_AAt[k], reverse=True)[:self.rank_AAt]
        self.U = np.zeros(shape=(self.MAX_USERS,self.rank_AAt))
        self.eigen_values_AAt = self.eigen_values_AAt[::-1] 
        self.eigen_values_AAt = self.eigen_values_AAt[:self.rank_AAt]

        for i in range(self.rank_AAt):
            self.U[:,i] = self.eigen_vectors_AAt[:,self.index_AAt[i]]

    def findSigma(self):
        """ finds the sigma matrix for SVD decomposition
        """
        self.sigma = np.zeros(shape=(self.rank_AAt, self.rank_AAt))
        for i in range(self.rank_AAt):
            self.sigma[i,i] = self.eigen_values_AAt[i] ** 0.5

    def findV(self):
        """ finds the V matrix for SVD decomposition
        """
        A = np.array(self.matrix)
        AtA = (A.T).dot(A)
        self.eigen_values_AtA, self.eigen_vectors_AtA = np.linalg.eigh(AtA)
        self.rank_AtA = np.linalg.matrix_rank(AtA)
        self.index_AtA = sorted(range(len(self.eigen_values_AtA)), key=lambda k: self.eigen_values_AtA[k], reverse=True)[:self.rank_AtA]
        self.V = np.zeros(shape=(self.MAX_ITEMS,self.rank_AtA))
        self.eigen_values_AtA = self.eigen_values_AtA[::-1]
        self.eigen_values_AtA = self.eigen_values_AtA[:self.rank_AtA]

        for i in range(self.rank_AtA):
            self.V[:,i] = self.eigen_vectors_AtA[:,self.index_AtA[i]]
        self.V = self.V.T

    def tryDimReduction(self):
        """ Tries to reduce the dimension of the matrics by following 90 %
            retain energy rule.
        """
        while True:
            total_E = 0
            size = np.shape(self.sigma)[0]
            for i in range(size):
                total_E += self.sigma[i,i]**2
            retained_E = 0
            if size > 0:
                retained_E = total_E - self.sigma[size-1,size-1]**2
            if total_E == 0 or retained_E/total_E < 0.9:
                break
            else:
                self.U = self.U[:,:-1:]
                self.V = self.V[:-1,:]
                self.sigma = self.sigma[:,:-1]
                self.sigma = self.sigma[:-1,:]

    def multiply(self):
        """ Multiplies the U, V and sigma matrices to get the matrix of predicted ratings.
        """
        self.result = np.dot((np.dot(self.U, self.sigma)), self.V)

N_users = 1000
N_movies = 1000
matrix = np.zeros(shape=(N_users, N_movies))
for i in range(N_users):
    for j in range(N_movies):
        if i in n_ratings and j in n_ratings[i]:
            matrix[i-1][j-1] = n_ratings[i][j]
            
start_time = time.time()
svd = SVD(matrix)
pred = svd.result
rmse = RMSE_mat(pred,matrix)
mae = MAE(pred,matrix)
end_time = time.time()
average_time  = (end_time - start_time) / (N_users * N_movies)
print('rmse : ' ,rmse)
print('mae : ',mae)
print('number of predictions made',(N_users * N_movies))
print('average time per prediction',average_time)
print('total time',(end_time - start_time))

print(svd.sigma)





"""# CUR Matrix Decomposition - Top K"""

class CUR_topk:
    """ Class to implement CUR Decomposition
    """ 
    def __init__(self, matrix, K):
        self.K = K
        self.matrix = np.array(matrix)
        self.numRows = np.shape(self.matrix)[0]
        self.numCols = np.shape(self.matrix)[1]
        self.getProbDistribution()
        self.findR(1,2)
        self.findC(1,2)
        self.findU()
        self.multiply()

    def getProbDistribution(self):
        """ Gets the probability distribution for the rows and columns to be used in 
            random list generation.
        """
        self.totalSum = sum(sum(self.matrix ** 2))
        self.rowSum = (sum((self.matrix.T)**2))/self.totalSum
        self.colSum = (sum(self.matrix ** 2))/self.totalSum

    def generateRandomNos(self, probDist, size, sampleSize, choice):
        """ The method generates sampleSsize number of random numbers which are in the range of 
            size. It samples out them using the given probability distribution
        """
        if choice == 0: return np.random.choice(np.arange(0,size), sampleSize, p=probDist)
        else: return np.random.choice(np.arange(0,size), sampleSize, replace=False,p=probDist)

    def findC(self, choice, sampleSize):
        """ Method to compute C matrix in the CUR decomposition 
        """
        rand_no = self.generateRandomNos(self.colSum, self.numCols, 2, choice)
        self.c_indices = rand_no
        self.C = (self.matrix[:,rand_no]).astype(float)
        colIdx, idx = 0, 0
        while colIdx < sampleSize:
            for rowIdx in range(0, self.numRows):
                self.C[rowIdx, colIdx] /= (sampleSize*self.colSum[rand_no[idx]])**0.5
            idx += 1
            colIdx += 1

    def findR(self, choice, sampleSize):
        """ Method to compute R matrix in the CUR decomposition 
        """
        rand_no = self.generateRandomNos(self.rowSum, self.numRows, 2, choice)
        self.R = (self.matrix[rand_no,:]).astype(float)
        rowIdx, idx = 0, 0
        while rowIdx < sampleSize:
            for colIdx in range(0, self.numCols):
                self.R[rowIdx, colIdx] /= (sampleSize*self.rowSum[rand_no[idx]])**0.5
            idx += 1
            rowIdx += 1

    def findU(self):
        """ Method to compute U matrix in the CUR decomposition 
        """
        self.U = self.R[:,self.c_indices]
        svd = SVD_topk(self.U, self.K)
        Y = svd.V.T
        sigma_sq_plus = 1/(svd.sigma ** 2)
        sigma_sq_plus[sigma_sq_plus == np.inf] = 0
        sigma_sq_plus[sigma_sq_plus == -np.inf] = 0
        sigma_sq_plus[sigma_sq_plus == np.nan] = 0
        X = svd.U.T
        self.U = np.dot(Y,np.dot(sigma_sq_plus,X))
        self.U[self.U == np.inf] = 0
        self.U[self.U == -np.inf] = 0
        self.U[self.U == np.nan] = 0

    def multiply(self):
        """ Multiplies the U, V and sigma matrices to get the matrix of predicted ratings.
        """
        self.result = np.dot((np.dot(self.C, self.U)), self.R)
        self.result[self.result == np.inf] = 0
        self.result[self.result == -np.inf] = 0
        self.result[self.result == np.nan] = 0

N_users = 500
N_movies = 500
matrix = np.zeros(shape=(N_users, N_movies))
for i in range(N_users):
    for j in range(N_movies):
        if i in n_ratings and j in n_ratings[i]:
            matrix[i-1][j-1] = n_ratings[i][j]
            
start_time = time.time()
svd = CUR_topk(matrix,7)
pred = svd.result
rmse = RMSE_mat(pred,matrix)
mae = MAE(pred,matrix)
end_time = time.time()
average_time  = (end_time - start_time) / (N_users * N_movies)
print('rmse : ' ,rmse)
print('mae : ',mae)
print('number of predictions made',(N_users * N_movies))
print('average time per prediction',average_time)
print('total time',(end_time - start_time))

cur.C

cur.U

cur.R

"""# CUR Matrix Decomposition - 90%"""

class CUR:
    """ Class to implement CUR Decomposition
    """ 
    def __init__(self, matrix):
        self.matrix = np.array(matrix)
        self.numRows = np.shape(self.matrix)[0]
        self.numCols = np.shape(self.matrix)[1]
        self.getProbDistribution()
        self.findR(1,2)
        self.findC(1,2)
        self.findU()
        self.multiply()

    def getProbDistribution(self):
        """ Gets the probability distribution for the rows and columns to be used in 
            random list generation.
        """
        self.totalSum = sum(sum(self.matrix ** 2))
        self.rowSum = (sum((self.matrix.T)**2))/self.totalSum
        self.colSum = (sum(self.matrix ** 2))/self.totalSum

    def generateRandomNos(self, probDist, size, sampleSize, choice):
        """ The method generates sampleSsize number of random numbers which are in the range of 
            size. It samples out them using the given probability distribution
        """
        if choice == 0: return np.random.choice(np.arange(0,size), sampleSize, p=probDist)
        else: return np.random.choice(np.arange(0,size), sampleSize, replace=False,p=probDist)

    def findC(self, choice, sampleSize):
        """ Method to compute C matrix in the CUR decomposition 
        """
        rand_no = self.generateRandomNos(self.colSum, self.numCols, 2, choice)
        self.c_indices = rand_no
        self.C = (self.matrix[:,rand_no]).astype(float)
        colIdx, idx = 0, 0
        while colIdx < sampleSize:
            for rowIdx in range(0, self.numRows):
                self.C[rowIdx, colIdx] /= (sampleSize*self.colSum[rand_no[idx]])**0.5
            idx += 1
            colIdx += 1

    def findR(self, choice, sampleSize):
        """ Method to compute R matrix in the CUR decomposition 
        """
        rand_no = self.generateRandomNos(self.rowSum, self.numRows, 2, choice)
        self.R = (self.matrix[rand_no,:]).astype(float)
        rowIdx, idx = 0, 0
        while rowIdx < sampleSize:
            for colIdx in range(0, self.numCols):
                self.R[rowIdx, colIdx] /= (sampleSize*self.rowSum[rand_no[idx]])**0.5
            idx += 1
            rowIdx += 1

    def findU(self):
        """ Method to compute U matrix in the CUR decomposition 
        """
        self.U = self.R[:,self.c_indices]
        svd = SVD(self.U)
        Y = svd.V.T
        sigma_sq_plus = 1/(svd.sigma ** 2)
        sigma_sq_plus[sigma_sq_plus == np.inf] = 0
        sigma_sq_plus[sigma_sq_plus == -np.inf] = 0
        sigma_sq_plus[sigma_sq_plus == np.nan] = 0
        X = svd.U.T
        self.U = np.dot(Y,np.dot(sigma_sq_plus,X))
        self.U[self.U == np.inf] = 0
        self.U[self.U == -np.inf] = 0
        self.U[self.U == np.nan] = 0

    def multiply(self):
        """ Multiplies the U, V and sigma matrices to get the matrix of predicted ratings.
        """
        self.result = np.dot((np.dot(self.C, self.U)), self.R)
        self.result[self.result == np.inf] = 0
        self.result[self.result == -np.inf] = 0
        self.result[self.result == np.nan] = 0

N_users = 500
N_movies = 500
matrix = np.zeros(shape=(N_users, N_movies))
for i in range(N_users):
    for j in range(N_movies):
        if i in n_ratings and j in n_ratings[i]:
            matrix[i-1][j-1] = n_ratings[i][j]
            
start_time = time.time()
svd = CUR(matrix)
pred = svd.result
rmse = RMSE_mat(pred,matrix)
mae = MAE(pred,matrix)
end_time = time.time()
average_time  = (end_time - start_time) / (N_users * N_movies)
print('rmse : ' ,rmse)
print('mae : ',mae)
print('number of predictions made',(N_users * N_movies))
print('average time per prediction',average_time)
print('total time',(end_time - start_time))



print(cur.C)

print(cur.U)

print(cur.R)

"""# LATENT FACTOR ANALYSIS"""

class LF():
    def __init__(self, R, K, alpha, beta, iterations):
        
        """
        Perform matrix factorization to predict empty
        entries in a matrix.

        Arguments
        - R (ndarray)   : user-item rating matrix
        - K (int)       : number of latent dimensions
        - alpha (float) : learning rate
        - beta (float)  : regularization parameter
        """

        self.R = R
        self.num_users, self.num_items = R.shape
        self.K = K
        self.alpha = alpha
        self.beta = beta
        self.iterations = iterations
        
    def train(self):
        # Initialize user and item latent feature matrice
        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))
        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))

        # Initialize the biases
        self.b_u = np.zeros(self.num_users)
        self.b_i = np.zeros(self.num_items)
        self.b = np.mean(self.R[np.where(self.R != 0)])

        # Create a list of training samples
        self.samples = [
            (i, j, self.R[i, j])
            for i in range(self.num_users)
            for j in range(self.num_items)
            if self.R[i, j] > 0
        ]
        # Perform stochastic gradient descent for number of iterations
        training_process = []
        for i in range(self.iterations):
            np.random.shuffle(self.samples)
            self.sgd()
            mse = self.mse()
            training_process.append((i, mse))
#             if (i+1) % 10 == 0:
#                 print("Iteration: %d ; error = %.4f" % (i+1, mse))

        

    def mse(self):
        """
        A function to compute the total mean square error
        """
        xs, ys = self.R.nonzero()
        predicted = self.full_matrix()
        error = 0
        cnt = 0
        for x, y in zip(xs, ys):
            error += pow(self.R[x, y] - predicted[x, y], 2)
            cnt += 1
    
        return np.sqrt(error / cnt )
    
    def mae(self):
        """
        A function to compute the total mean square error
        """
        xs, ys = self.R.nonzero()
        predicted = self.full_matrix()
        error = 0
        cnt = 0
        for x, y in zip(xs, ys):
            error += abs(self.R[x, y] - predicted[x, y])
            cnt += 1
        return error / cnt
    
    def sgd(self):
        """
        Perform stochastic graident descent
        """
        for i, j, r in self.samples:
            # Computer prediction and error
            prediction = self.get_rating(i, j)
            e = (r - prediction)

            # Update biases
            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])
            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])

            # Update user and item latent feature matrices
            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])
            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])
    
    def get_rating(self, i, j):
        """
        Get the predicted rating of user i and item j
        """
        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)
        return prediction

    def full_matrix(self):
        """
        Computer the full matrix using the resultant biases, P and Q
        """
        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)

N_users = 1000
N_movies = 1000
R = np.zeros(shape=(N_users+1,N_movies+1))
for uid in range(1,N_users+1):
    for mid in range(1,N_movies+1):
        if mid in _ratings[uid]:
            R[uid - 1][mid - 1] = _ratings[uid][mid]
start_time = time.time()
lf = LF(R,15,0.1,0.1,20)
lf.train()
mse = lf.mse()
mae = lf.mae()
end_time = time.time()
average_time  = (end_time - start_time) / (N_users * N_movies)
print('rmse : ' ,mse)
print('mae : ',mae)
print('number of predictions made',(N_users * N_movies))
print('average time per prediction',average_time)
print('total time',(end_time - start_time))

#experiment with different values of K
for k in range(1,4):
    lf = LF(R,10*k,0.1,0.1,20)
    lf.train()
    print(k * 10,lf.mse(),lf.mae())

